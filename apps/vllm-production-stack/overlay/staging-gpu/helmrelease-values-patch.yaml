---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vllm-stack
  namespace: flux-system
spec:
  chart:
    spec:
      version: "0.1.7"
      interval: 15m
  values:
    # Unified configuration for disaggregated prefill setup
    fullnameOverride: vllm-stack
    servingEngineSpec:
      enableEngine: true
      runtimeClassName: "nvidia-container-runtime"
      containerPort: 8000
      # -- Startup probe configuration
      startupProbe:
        initialDelaySeconds: 15
        periodSeconds: 10
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
      # -- Liveness probe configuration
      livenessProbe:
          initialDelaySeconds: 15
          failureThreshold: 3
          periodSeconds: 10
          httpGet:
            path: /health
            port: 8000
      # -- Readiness probe configuration
      readinessProbe:
            initialDelaySeconds: 15
            failureThreshold: 3
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8000
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      modelSpec:
        # Prefill node configuration
        - name: "llama-prefill"
          repository: "lmcache/vllm-openai"
          tag: "2025-05-17-v1"
          modelURL: "meta-llama/Llama-3.1-8B-Instruct"
          replicaCount: 1
          requestCPU: 8
          requestMemory: "30Gi"
          requestGPU: 1
          requestGPUType: nvidia.com/mig-2g.24gb
          pvcStorage: "50Gi"
          vllmConfig:
            enableChunkedPrefill: false
            enablePrefixCaching: false
            maxModelLen: 8000
            v1: 1
          lmcacheConfig:
            enabled: true
            kvRole: "kv_producer"
            enableNixl: true
            nixlRole: "sender"
            nixlPeerHost: "pd-llama-decode-engine-service"
            nixlPeerPort: "55555"
            nixlBufferSize: "1073741824"  # 1GB
            nixlBufferDevice: "cuda"
            nixlEnableGc: true
            enablePD: true
            cpuOffloadingBufferSize: 0
          hf_token:
            secretName: "hf-token"
            secretKey: "HUGGING_FACE_HUB_TOKEN"
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    - key: nvidia.com/mig.capable
                      operator: Exists
          labels:
            model: "llama-prefill"
        # Decode node configuration
        - name: "llama-decode"
          repository: "lmcache/vllm-openai"
          tag: "2025-05-17-v1"
          modelURL: "meta-llama/Llama-3.1-8B-Instruct"
          replicaCount: 1
          requestCPU: 8
          requestMemory: "30Gi"
          requestGPU: 1
          requestGPUType: nvidia.com/mig-2g.24gb
          pvcStorage: "50Gi"
          vllmConfig:
            enableChunkedPrefill: false
            enablePrefixCaching: false
            maxModelLen: 8000
            v1: 1
          lmcacheConfig:
            enabled: true
            kvRole: "kv_consumer"
            enableNixl: true
            nixlRole: "receiver"
            nixlPeerHost: "0.0.0.0"
            nixlPeerPort: "55555"
            nixlBufferSize: "1073741824"  # 1GB
            nixlBufferDevice: "cuda"
            nixlEnableGc: true
            enablePD: true
          hf_token:
            secretName: "hf-token"
            secretKey: "HUGGING_FACE_HUB_TOKEN"
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                    - key: nvidia.com/mig.capable
                      operator: Exists
          labels:
            model: "llama-decode"
    routerSpec:
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      enableRouter: true
      repository: "lmcache/lmstack-router"
      tag: "pd-05-26"
      replicaCount: 1
      containerPort: 8000
      servicePort: 80
      routingLogic: "disaggregated_prefill"
      engineScrapeInterval: 15
      requestStatsWindow: 60
      enablePD: true
      resources:
        requests:
          cpu: "3"
          memory: "6G"
        limits:
          cpu: "3"
          memory: "10G"
      labels:
        environment: "router"
        release: "router"
      extraArgs:
        - "--prefill-model-labels"
        - "llama-prefill"
        - "--decode-model-labels"
        - "llama-decode"
      # Liveness probe configuration.
      livenessProbe:
        initialDelaySeconds: 30
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health
      # Startup probe configuration.
      startupProbe:
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health
      # Readiness probe configuration.
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health
      # nodeSelectorTerms:
      #   - matchExpressions:
      #     - key: nvidia.com/mig.capable
      #       operator: Exists
      # affinity:
      #   nodeAffinity:
      #     requiredDuringSchedulingIgnoredDuringExecution:
      #       nodeSelectorTerms:
      #           - key: nvidia.com/mig.capable
      #             operator: Exists