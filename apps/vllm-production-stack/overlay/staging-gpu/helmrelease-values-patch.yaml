---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vllm-production-stack
  namespace: flux-system
spec:
  chart:
    spec:
      version: "0.1.7"
  values:
    # Unified configuration for disaggregated prefill setup
    servingEngineSpec:
      enableEngine: true
      runtimeClassName: "nvidia"
      containerPort: 8000
      # -- Startup probe configuration
      startupProbe:
        initialDelaySeconds: 15
        periodSeconds: 10
        failureThreshold: 60
        httpGet:
          path: /health
          port: 8000
      # -- Liveness probe configuration
      livenessProbe:
          initialDelaySeconds: 15
          failureThreshold: 3
          periodSeconds: 10
          httpGet:
            path: /health
            port: 8000
      # -- Readiness probe configuration
      readinessProbe:
            initialDelaySeconds: 15
            failureThreshold: 3
            periodSeconds: 5
            httpGet:
              path: /health
              port: 8000
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Equal"
          value: "true"
          effect: "NoSchedule"
      modelSpec:
        # Prefill node configuration
        - name: "llama-prefill"
          repository: "lmcache/vllm-openai"
          tag: "2025-05-17-v1"
          modelURL: "meta-llama/Llama-3.1-8B-Instruct"
          replicaCount: 1
          requestCPU: 8
          requestMemory: "30Gi"
          requestGPU: 1
          pvcStorage: "50Gi"
          vllmConfig:
            enableChunkedPrefill: false
            enablePrefixCaching: false
            maxModelLen: 32000
            v1: 1
          lmcacheConfig:
            enabled: true
            kvRole: "kv_producer"
            enableNixl: true
            nixlRole: "sender"
            nixlPeerHost: "pd-llama-decode-engine-service"
            nixlPeerPort: "55555"
            nixlBufferSize: "1073741824"  # 1GB
            nixlBufferDevice: "cuda"
            nixlEnableGc: true
            enablePD: true
            cpuOffloadingBufferSize: 0
          hfToken:
            secretName: "hf-token"
            secretKey: "HUGGING_FACE_HUB_TOKEN"
          nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/mig.capable
                operator: "In"
                values:
                - "true"
          labels:
            model: "llama-prefill"
        # Decode node configuration
        - name: "llama-decode"
          repository: "lmcache/vllm-openai"
          tag: "2025-05-17-v1"
          modelURL: "meta-llama/Llama-3.1-8B-Instruct"
          replicaCount: 1
          requestCPU: 8
          requestMemory: "30Gi"
          requestGPU: 1
          pvcStorage: "50Gi"
          vllmConfig:
            enableChunkedPrefill: false
            enablePrefixCaching: false
            maxModelLen: 32000
            v1: 1
          lmcacheConfig:
            enabled: true
            kvRole: "kv_consumer"
            enableNixl: true
            nixlRole: "receiver"
            nixlPeerHost: "0.0.0.0"
            nixlPeerPort: "55555"
            nixlBufferSize: "1073741824"  # 1GB
            nixlBufferDevice: "cuda"
            nixlEnableGc: true
            enablePD: true
          hfToken:
            secretName: "hf-token"
            secretKey: "HUGGING_FACE_HUB_TOKEN"
          nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/mig.capable
                operator: "In"
                values:
                - "true"
          labels:
            model: "llama-decode"
    routerSpec:
      enableRouter: true
      repository: "lmcache/lmstack-router"
      tag: "pd-05-26"
      replicaCount: 1
      containerPort: 8000
      servicePort: 80
      routingLogic: "disaggregated_prefill"
      engineScrapeInterval: 15
      requestStatsWindow: 60
      enablePD: true
      resources:
        requests:
          cpu: "4"
          memory: "16G"
        limits:
          cpu: "4"
          memory: "32G"
      labels:
        environment: "router"
        release: "router"
      extraArgs:
        - "--prefill-model-labels"
        - "llama-prefill"
        - "--decode-model-labels"
        - "llama-decode"
      # Liveness probe configuration.
      livenessProbe:
        initialDelaySeconds: 30
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health
      # Startup probe configuration.
      startupProbe:
        initialDelaySeconds: 5
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health
      # Readiness probe configuration.
      readinessProbe:
        initialDelaySeconds: 30
        periodSeconds: 5
        failureThreshold: 3
        httpGet:
          path: /health